{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a81f04a-501e-4edc-8376-2b212f058640",
      "metadata": {
        "id": "5a81f04a-501e-4edc-8376-2b212f058640",
        "tags": []
      },
      "source": [
        "# Parámetros globales del *Notebook*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "dafe6cb7-f4e8-4aa1-8368-19c0bf273dac",
      "metadata": {
        "id": "dafe6cb7-f4e8-4aa1-8368-19c0bf273dac"
      },
      "outputs": [],
      "source": [
        "# Para definir los path\n",
        "import os\n",
        "\n",
        "# Define si estamos ejecutando el Notebook en nuestro \n",
        "# ordenador (\"local\") o en Google Colab (\"remote\")\n",
        "RUNNING_ENV = \"local\"\n",
        "\n",
        "# Path que vamos a usar como base para el resto de paths\n",
        "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# Directorio en el que guardamos los scripts de python que usamos \n",
        "# como libreria propia\n",
        "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
        "\n",
        "# Directorio en el que guardamos los datos de entrenamiento y test\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
        "\n",
        "# Numero de procesos que queremos usar\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Batch size que queremos usar para los dataloaders que usamos\n",
        "DATALOADER_BACH_SIZE = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81fd5e8-320b-4e00-be57-4bdf5799dad1",
      "metadata": {
        "id": "a81fd5e8-320b-4e00-be57-4bdf5799dad1"
      },
      "source": [
        "# Autorización si estamos usando Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "6fcac799-d53f-4e6d-aaf4-4e7df3ae7422",
      "metadata": {
        "id": "6fcac799-d53f-4e6d-aaf4-4e7df3ae7422"
      },
      "outputs": [],
      "source": [
        "if RUNNING_ENV == \"remote\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638c766e-6f30-46d7-b6d3-97a6b7e30995",
      "metadata": {
        "id": "638c766e-6f30-46d7-b6d3-97a6b7e30995"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "3aeb81a8-78a0-4e63-aa9d-3c82d24100b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aeb81a8-78a0-4e63-aa9d-3c82d24100b5",
        "outputId": "7f284ae0-e443-4a74-e410-38fa5e20865e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat './lib/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Cargamos en el Notebook todos los ficheros .py que definen nuestra propia libreria\n",
        "# Usamos esta libreria para escribir el codigo base necesario para llevar a cabo ciertas\n",
        "# tareas del notebook (como el bucle de entrenamiento) que no tienen interes mostrar\n",
        "# en este notebook\n",
        "!cp -r \"$LIB_PATH\"/* .\n",
        "\n",
        "# Ahora que hemos cargado estos ficheros en el Notebook, importamos lo necesario\n",
        "# de nuestra propia libreria\n",
        "import core\n",
        "import board\n",
        "import filesystem\n",
        "from train_loggers import ClassificationLogger, SilentLogger, TripletLogger\n",
        "from models.resnet import *\n",
        "from visualizations import *\n",
        "from custom_loss import triplet_loss_batch_hard"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4cbeab3-125d-44e4-8734-928f8f0c664c",
      "metadata": {
        "id": "b4cbeab3-125d-44e4-8734-928f8f0c664c"
      },
      "source": [
        "# Carga del conjunto de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "faa50ba8-0f5f-420e-bcbc-cfffc165ee44",
      "metadata": {
        "id": "faa50ba8-0f5f-420e-bcbc-cfffc165ee44"
      },
      "outputs": [],
      "source": [
        "# Transformaciones que queremos aplicar al cargar los datos\n",
        "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
        "])\n",
        "\n",
        "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
        "# preparado para descargar\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "# Data loaders para acceder a los datos\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = DATALOADER_BACH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size = DATALOADER_BACH_SIZE,\n",
        "  shuffle = True,\n",
        "  num_workers = NUM_WORKERS,\n",
        "  pin_memory = True,\n",
        ")\n",
        "\n",
        "# Class that we are going to work with\n",
        "classes = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d5b0ca-5a5a-4a4b-b0f4-dd6cf021f73d",
      "metadata": {
        "id": "84d5b0ca-5a5a-4a4b-b0f4-dd6cf021f73d",
        "tags": []
      },
      "source": [
        "# Análisis Exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3a17a3-c502-41ef-8197-b0c6989e9402",
      "metadata": {
        "id": "7b3a17a3-c502-41ef-8197-b0c6989e9402"
      },
      "source": [
        "Mostramos algunas imágenes con sus clases para asegurar que hemos cargado correctamente las imágenes del conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "816b14ed-0a49-44f5-ac5b-16dce9e58091",
      "metadata": {
        "id": "816b14ed-0a49-44f5-ac5b-16dce9e58091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f22c4f42-2564-44a0-f046-27606b5032c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit obtained is 0\n",
            "Img shape is torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit obtained is 1\n",
            "Img shape is torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit obtained is 5\n",
            "Img shape is torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit obtained is 2\n",
            "Img shape is torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Digit obtained is 6\n",
            "Img shape is torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "imgs_to_show = 5\n",
        "\n",
        "for _ in range(imgs_to_show):\n",
        "    images, digit_classes = next(iter(train_loader))\n",
        "    img, digit_class = images[0], digit_classes[0]\n",
        "    print(f\"Digit obtained is {digit_class}\")\n",
        "    print(f\"Img shape is {img.shape}\")\n",
        "\n",
        "    # Reshape and plot the img\n",
        "    img = img.reshape((28, 28))\n",
        "    show_img(img, color_format_range = (-1.0, 1.0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11747bce-2a7e-4c9c-bcc0-d22989a6faf0",
      "metadata": {
        "id": "11747bce-2a7e-4c9c-bcc0-d22989a6faf0"
      },
      "source": [
        "Now we can show more than one image at a time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "e8225574-65fa-49c9-87d9-91b3d19bdef7",
      "metadata": {
        "id": "e8225574-65fa-49c9-87d9-91b3d19bdef7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "39855298-0d4c-4ce0-b805-67153c41a0a4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-b17aa2cd8b91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigit_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mshow_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor_format_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: show_images() got an unexpected keyword argument 'figsize'"
          ]
        }
      ],
      "source": [
        "images, digit_classes = next(iter(train_loader))\n",
        "images = [img.reshape((28, 28)) for img in images]\n",
        "show_images(images, color_format_range = (-1.0, 1.0), columns = 4, figsize = (20, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a73b156-19f0-470e-ad06-e140bc65a61c",
      "metadata": {
        "id": "0a73b156-19f0-470e-ad06-e140bc65a61c"
      },
      "source": [
        "Now lets see some statistics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "f1f3dcc8-0482-446d-a686-e4e7ca9b6afd",
      "metadata": {
        "id": "f1f3dcc8-0482-446d-a686-e4e7ca9b6afd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4eb69a7-695e-48bd-8462-f0e722d7323e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 60000 train samples\n",
            "We have 10000 test samples\n"
          ]
        }
      ],
      "source": [
        "print(f\"We have {len(train_dataset)} train samples\")\n",
        "print(f\"We have {len(test_dataset)} test samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e33621e-25d4-44ad-adba-07cd26e2c674",
      "metadata": {
        "id": "1e33621e-25d4-44ad-adba-07cd26e2c674"
      },
      "source": [
        "## Conclusions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bf6d77-999c-49fc-9674-307c6a9d0820",
      "metadata": {
        "id": "99bf6d77-999c-49fc-9674-307c6a9d0820"
      },
      "source": [
        "- We are working with $28 x 28 x 1$ images\n",
        "- We have to work with ten classes\n",
        "- 60k training samples and 10k test samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9d19cf8-9369-4d9b-a28b-3d59fda64bc4",
      "metadata": {
        "id": "f9d19cf8-9369-4d9b-a28b-3d59fda64bc4"
      },
      "source": [
        "# Defining the baseline model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "5153ec0e-fa9f-4335-aa31-bb424ae59122",
      "metadata": {
        "id": "5153ec0e-fa9f-4335-aa31-bb424ae59122"
      },
      "outputs": [],
      "source": [
        "class BaseLine(nn.Module):\n",
        "    \"\"\"\n",
        "    Baseline model used to compare it with more advanced models\n",
        "    Flatten the input and apply FC layers. Batch normalization enhanced a lot model performance (92% acc to 97% acc)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        # Init parent class\n",
        "        super().__init__()\n",
        "        \n",
        "        # Architecture:\n",
        "        self.fc1 = nn.Linear(28*28, 100)\n",
        "        self.bn1 = nn.BatchNorm1d(100)\n",
        "        \n",
        "        self.fc2 = nn.Linear(100, 150)\n",
        "        self.bn2 = nn.BatchNorm1d(150)\n",
        "\n",
        "        self.fc3 = nn.Linear(150, 10)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Flatten the input\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        \n",
        "        # First layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Second layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "905faa72-9a5a-48d9-ae70-405b69ef1c6f",
      "metadata": {
        "id": "905faa72-9a5a-48d9-ae70-405b69ef1c6f"
      },
      "source": [
        "# LuNet Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "81a157dc-fe12-46d4-9369-84514c80486c",
      "metadata": {
        "id": "81a157dc-fe12-46d4-9369-84514c80486c"
      },
      "outputs": [],
      "source": [
        "class LuNet(nn.Module):\n",
        "    \"\"\"\n",
        "    LuNet model adapted to 28x28x1 input images\n",
        "    \n",
        "    Based on the model presented in the paper \"In defense of the triplet loss \n",
        "    for person re-identification\"\n",
        "    \n",
        "    In order to make some experiments with this model\n",
        "    \n",
        "    Original architecture is quite similar to a common ResNet using bottleneck \n",
        "    building blocks with batch normalization. The real work happens in the \n",
        "    loss function and also the way triplets are chosen. So we are going to \n",
        "    stack some building blocks without max-pooling (28x28x1 is small enough)\n",
        "    and apply that loss function and triplet selection policy\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):          \n",
        "        # Init parent class\n",
        "        super().__init__()\n",
        "\n",
        "        # ResNet Blocks\n",
        "        self.block1 = BottleNeckBlock(\n",
        "            input_channels = 1,\n",
        "            output_channels = 8,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            disable_identity = True,\n",
        "        )\n",
        "        \n",
        "        self.block2 = BottleNeckBlock(\n",
        "            input_channels = 8,\n",
        "            output_channels = 8,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            disable_identity = False,\n",
        "        )\n",
        "        \n",
        "        self.block3 = BottleNeckBlock(\n",
        "            input_channels = 8,\n",
        "            output_channels = 16,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            disable_identity = True,\n",
        "        )\n",
        "        \n",
        "        self.block4 = BottleNeckBlock(\n",
        "            input_channels = 16,\n",
        "            output_channels = 16,\n",
        "            kernel_size = 3,\n",
        "            stride = 1,\n",
        "            disable_identity = False,\n",
        "        )\n",
        "        \n",
        "        # Fully connected layers\n",
        "        # We end up with an embedding space of dimension 64\n",
        "        self.fc1 = nn.Linear(12544, 128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # ResNet blocks\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.block3(x)\n",
        "        \n",
        "        # Now fully-connected layers\n",
        "        x = torch.flatten(x, start_dim = 1)\n",
        "        \n",
        "        x = self.fc1(x)\n",
        "        # TODO -- enable this again\n",
        "        #x = self.bn1(x)\n",
        "        x = F.relu(x)        \n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d32f0f2-bc99-4436-a004-53d1b8e64036",
      "metadata": {
        "id": "3d32f0f2-bc99-4436-a004-53d1b8e64036"
      },
      "source": [
        "# Custom Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95dc3d4-6415-4869-8d1f-0e2b6f3917cb",
      "metadata": {
        "id": "e95dc3d4-6415-4869-8d1f-0e2b6f3917cb"
      },
      "source": [
        "`TripletMarginLoss` requires three parameters: anchor, positive and negative. We can change the training loop for, given a batch of examples and their labels, calculate a triplet (we are going to end with batch hard triplets) and pass that to the loss func. Also we can make it generalized using lambda functions as parameters (more strategy pattern in training loop). \n",
        "\n",
        "Or we can do better and create a custom data loader that handles the creation of hard triplets, batches of hard triplets and left the training loop as it was. Fortunately, pytorch makes very easy and convinient to create custom data loaders. \n",
        "\n",
        "Inspired by [this notebook](https://www.kaggle.com/hirotaka0122/triplet-loss-with-pytorch#Define-class-MNIST). Our dataloader is slightly different (mainly the *batch hard triplet concept*)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52123ca0-e4d8-4b3f-b759-7e69b85e89a4",
      "metadata": {
        "id": "52123ca0-e4d8-4b3f-b759-7e69b85e89a4"
      },
      "source": [
        "## Random triplets\n",
        "\n",
        "We well start by creating a custom data set that, given an anchor by its index, gets a random positive anda a random negative for that anchor. Its a good starting point but it's not enough, as said in the paper we're basing our work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "339b21d9-eaee-484c-83ac-5385609e05d5",
      "metadata": {
        "id": "339b21d9-eaee-484c-83ac-5385609e05d5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MNIST_RANDOM_TRIPLET(Dataset):\n",
        "    \"\"\"\n",
        "    Custom MNIST data loader that returns batch random triplets instead of input + label\n",
        "    That means we select the anchor via the index, and then a positive and a negative is chosen randomly\n",
        "    \n",
        "    Under the hood, we are using pytorch mnist dataset, but creating the triplets\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, train: bool = True, root: str = \"./data\", transform=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset. Uses mnist dataset stored in pytorch\n",
        "        \n",
        "        Parameters:\n",
        "        ============\n",
        "        train: if we are loading the training set (True) or the test set (False)\n",
        "        root: path where to save the dataset\n",
        "        transform: transforms we want to apply to the images stored before returning them\n",
        "        \"\"\"\n",
        "        \n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        \n",
        "        # Our images storage \n",
        "        self.underlying_dataset = None\n",
        "        if self.train is True:\n",
        "            self.underlying_dataset = datasets.MNIST(root= root, train=True, download=True, transform=self.transform)\n",
        "        else:\n",
        "            self.underlying_dataset = datasets.MNIST(root= root, train=False, download=True, transform=self.transform)\n",
        "            \n",
        "        # We compute classes indixes here to save computation time in training loop\n",
        "        self.indixes_of_classes = [self.__create_index_list(index) for index in range(0, 10)]\n",
        "        self.indixes_of_classes = np.array(self.indixes_of_classes)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.underlying_dataset)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\"\n",
        "        Returns an item in the form of anchor, positive, negative image\n",
        "        Anchor is given by the index, positive and negative chosen randomly\n",
        "        \n",
        "        Parameters:\n",
        "        ============\n",
        "        idx: index of the item we are looking for\n",
        "        \n",
        "        Returns:\n",
        "        ========\n",
        "        anchor: the idx-th image of the dataset\n",
        "        positive: some image with the same idx of the image\n",
        "        negative: some image with is no same idx of the image\n",
        "        \"\"\"\n",
        "        \n",
        "        # Convert tensor index into list index\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Get the item from the dataset\n",
        "        # Anchor is the image of given idx\n",
        "        anchor, anchor_label = self.underlying_dataset[idx]\n",
        "                \n",
        "        # Now get some random positive fron the dataset\n",
        "        positive_index = np.random.choice(self.indixes_of_classes[anchor_label][0])\n",
        "        positive, _ = self.underlying_dataset[positive_index]\n",
        "        \n",
        "        # Now we get some random negative from the dataset\n",
        "        negative_classes = np.delete(self.indixes_of_classes, [anchor_label])\n",
        "        negative_class_indixes = np.random.choice(negative_classes)\n",
        "        negative_index = np.random.choice(negative_class_indixes)\n",
        "        negative, _ = self.underlying_dataset[negative_index]\n",
        "   \n",
        "        return anchor, positive, negative\n",
        "\n",
        "    def __create_index_list(self, label: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Creates an array of indixes of the positions of certain label in the dataset\n",
        "        For example: given class 0 returns np.array(3, 4, 9) because in these positions\n",
        "        there are images matching that label\n",
        "        \"\"\"\n",
        "\n",
        "        labels = self.underlying_dataset.targets\n",
        "        indixes = torch.where(labels == label)\n",
        "        indixes = indixes\n",
        "        return indixes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06bc18a2-579c-4bc8-b831-83b10e100c91",
      "metadata": {
        "id": "06bc18a2-579c-4bc8-b831-83b10e100c91"
      },
      "source": [
        "Lets test this showing some images obtained through the custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "c35f4c3a-102e-4929-9b57-3d3334e25187",
      "metadata": {
        "id": "c35f4c3a-102e-4929-9b57-3d3334e25187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "4e529536-d84e-47d2-af41-0644f85f6072"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-2c9cfe3f64be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Create the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNIST_RANDOM_TRIPLET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load some images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-73-9e60889a0e2c>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train, root, transform)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# We compute classes indixes here to save computation time in training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindixes_of_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__create_index_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindixes_of_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindixes_of_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
          ]
        }
      ],
      "source": [
        "# Convert PIL images to tensor\n",
        "# ALso normalize images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "dataset = MNIST_RANDOM_TRIPLET(train = True, transform = transform)\n",
        "\n",
        "# Load some images\n",
        "anchor, positive, negative = dataset[0]\n",
        "anchor, positive, negative = anchor.reshape(28,28), positive.reshape(28,28), negative.reshape(28,28)\n",
        "show_images([anchor, positive, negative], color_format_range = (-1.0, 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fbe0d2-a162-4cf3-a667-69cfaee54fa5",
      "metadata": {
        "id": "49fbe0d2-a162-4cf3-a667-69cfaee54fa5"
      },
      "source": [
        "And now I override classic dataloader with our custom dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e880d4-9c50-4dc7-a27f-69551fb102f5",
      "metadata": {
        "id": "62e880d4-9c50-4dc7-a27f-69551fb102f5"
      },
      "outputs": [],
      "source": [
        "# Convert PIL images to tensor\n",
        "# ALso normalize images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "# Create the dataset\n",
        "training_set = MNIST_RANDOM_TRIPLET(train = True, transform = transform)\n",
        "testing_set = MNIST_RANDOM_TRIPLET(train = False, transform = transform)\n",
        "\n",
        "\n",
        "# Data loaders for accessing the data\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    training_set, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory = True,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(testing_set, batch_size = batch_size, shuffle = True, num_workers = num_workers, pin_memory = True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12ccc27a-9778-4a8c-93ed-4d0b96a99ece",
      "metadata": {
        "id": "12ccc27a-9778-4a8c-93ed-4d0b96a99ece"
      },
      "source": [
        "# Batch Hard Triplets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447d837f-6461-4720-8656-924b33c4587b",
      "metadata": {
        "id": "447d837f-6461-4720-8656-924b33c4587b"
      },
      "source": [
        "This time we are going to implement a dataset for mining batch hard triplets. In order to implement this, we have to:\n",
        "\n",
        "- Randomly sample P classes\n",
        "- Randomly sample K images for each P classes\n",
        "- For each image, compute loss using hardest positive and hardest negative\n",
        "\n",
        "So, when accessing the dataset using index i-th, we are going to generate a PK batch. So the dataloader has to use batch size 1, because we're doing the batch working directly through the dataset (I tried to do this trought the dataloader but failed to do so)\n",
        "\n",
        "Also, the len of the dataset is going to set by the user of the dataset, because we're randomly sampling batches, thus we can set `__len__`as we want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e933db97-fea4-4e84-b54f-2c6162fc0d6a",
      "metadata": {
        "id": "e933db97-fea4-4e84-b54f-2c6162fc0d6a"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class MNIST_HARDBATCH_TRIPLET(Dataset):\n",
        "    \"\"\"\n",
        "    Custom MNIST data loader that returns batch hard triplets instead of input + label\n",
        "    That means we are sampling P classes from the dataset. For each class we're sampling\n",
        "    K images. Therefore, we end with PK image. \n",
        "    \n",
        "    For each image in the PK class, we want to compute the triplet loss using the hardest\n",
        "    positive and the hardest negative. But this has to be handled in the training loop\n",
        "    \n",
        "    This dataset implements some kind of batching. Thus, the dataloader used with this\n",
        "    dataset has to use batch_size = 1\n",
        "    \n",
        "    Under the hood, we are using pytorch mnist dataset, but managing the triplets\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, train: bool = True, root: str = \"./data\", transform=None, num_classes: int = 4, images_per_class: int = 10, custom_len: int = 32):\n",
        "        \"\"\"\n",
        "        Initializes the dataset. Uses mnist dataset stored in pytorch\n",
        "        \n",
        "        Parameters:\n",
        "        ============\n",
        "        train: if we are loading the training set (True) or the test set (False)\n",
        "        root: path where to save the dataset\n",
        "        transform: transforms we want to apply to the images stored before returning them\n",
        "        num_classes: number of classes sampled at each batch\n",
        "        images_per_class: number of images sampled for each sampled class\n",
        "        custom_len: custom len, we are not using the original dataset len because we're randomly sampling images\n",
        "                    from the dataset, so any len can be specified. A good idea can be:\n",
        "                        custom_len = len(dataset) / (PK)\n",
        "        \"\"\"\n",
        "        \n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.num_classes = num_classes\n",
        "        self.images_per_class = images_per_class\n",
        "        self.custom_len = custom_len\n",
        "        \n",
        "        # Our images storage \n",
        "        self.underlying_dataset = None\n",
        "        if self.train is True:\n",
        "            self.underlying_dataset = datasets.MNIST(root= root, train=True, download=True, transform=self.transform)\n",
        "        else:\n",
        "            self.underlying_dataset = datasets.MNIST(root= root, train=False, download=True, transform=self.transform)\n",
        "            \n",
        "        # We compute classes indixes here to save computation time in training loop\n",
        "        self.indixes_of_classes = [self.__create_index_list(index) for index in range(0, 10)]\n",
        "        self.indixes_of_classes = np.array(self.indixes_of_classes)\n",
        "    \n",
        "    def __len__(self):\n",
        "        # We return the custom len that the user of this dataset specifies\n",
        "        return self.custom_len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\"\n",
        "        Returns a batch consisting of PK images, as describe in the class docs\n",
        "                \n",
        "        Parameters:\n",
        "        ============\n",
        "        idx: index of the item we are looking for\n",
        "             In this case, this index is useless\n",
        "        \n",
        "        Returns:\n",
        "        ========\n",
        "        training_batch: dict having P keys (class labels) and keys of K images for each class\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO -- not correct: Convert tensor index into list index\n",
        "        # Check if we are using batch size greater than 0\n",
        "        if torch.is_tensor(idx):\n",
        "            #idx = idx.tolist()\n",
        "            raise Exception(\"MNIST_HARDBATCH_TRIPLET don't support batch size greater than 1\")\n",
        "\n",
        "                \n",
        "        # Dict we are going to return\n",
        "        training_batch = dict()\n",
        "        \n",
        "        # Select P different random classes\n",
        "        classes = [x for x in range(0, 10)]\n",
        "        classes = np.random.choice(classes, size = self.num_classes, replace = False)\n",
        "        \n",
        "        # Select K randon images for each class\n",
        "        for curr_class in classes:\n",
        "            # Select random indixes of images belonging to this class\n",
        "            curr_images_idx = self.indixes_of_classes[curr_class][0]\n",
        "            curr_images = np.random.choice(curr_images_idx, size = self.images_per_class, replace = False)\n",
        "            \n",
        "            # Convert random indixes to their correspondent images\n",
        "            curr_images = [self.underlying_dataset[idx] for idx in curr_images]\n",
        "            \n",
        "            # We are using underlying dataset for getting images, so we get batches\n",
        "            # of size 1 when querying for images. Also reshape to have images of\n",
        "            # shape 28x28\n",
        "            curr_images = [img[0] for img in curr_images]\n",
        "            curr_images = [img.reshape(28, 28) for img in curr_images]\n",
        "            \n",
        "            # Set this to our dict\n",
        "            training_batch[curr_class] = curr_images\n",
        "        \n",
        "        return training_batch\n",
        "\n",
        "    def __create_index_list(self, label: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Creates an array of indixes of the positions of certain label in the dataset\n",
        "        For example: given class 0 returns np.array(3, 4, 9) because in these positions\n",
        "        there are images matching that label\n",
        "        \"\"\"\n",
        "\n",
        "        labels = self.underlying_dataset.targets\n",
        "        indixes = torch.where(labels == label)\n",
        "        indixes = indixes\n",
        "        return indixes\n",
        "    \n",
        "    def display_batch(batch: dict):\n",
        "        \"\"\"Gets a dict created by Self.__getitem__, and for each random sampled class,\n",
        "        displays some random images corresponding to that class\"\"\"\n",
        "        # Show some images from the batch\n",
        "        # Convert dict of images to single-line list\n",
        "        images = []\n",
        "        for key in batch:\n",
        "            curr_images = batch[key]\n",
        "            curr_images = curr_images[:5]\n",
        "            \n",
        "            # Just in case, it should not happen\n",
        "            curr_images = [img.reshape(28, 28) for img in curr_images]\n",
        "            \n",
        "            # Append to the image list\n",
        "            images = images + curr_images\n",
        "\n",
        "        show_images(images, color_format_range = (-1, 1), columns = 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d92e6e1-995e-4562-8d3b-fc9a6ddf1999",
      "metadata": {
        "id": "1d92e6e1-995e-4562-8d3b-fc9a6ddf1999"
      },
      "source": [
        "Lets see how it works the dataset works. For this, we create a random batch from the dataset, and show for each random sampled class, some images sampled for that class: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c23268a-7b70-40e5-b971-a87f69cc1b55",
      "metadata": {
        "id": "7c23268a-7b70-40e5-b971-a87f69cc1b55"
      },
      "outputs": [],
      "source": [
        "# First, generate the dataset\n",
        "training_set = MNIST_HARDBATCH_TRIPLET(\n",
        "    train = True, \n",
        "    root = \"./data\",\n",
        "    transform=transform,\n",
        "    num_classes = 4, \n",
        "    images_per_class = 10,\n",
        "    custom_len = 32\n",
        ")\n",
        "\n",
        "# Now, get a batch from the dataset\n",
        "batch = training_set[0]\n",
        "MNIST_HARDBATCH_TRIPLET.display_batch(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df58cf9e-51ba-4c88-8e29-b26a3319b1d7",
      "metadata": {
        "id": "df58cf9e-51ba-4c88-8e29-b26a3319b1d7"
      },
      "source": [
        "Now we are going to define datasets and dataloaders for training and testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "283367c5-4134-4378-bb27-829b0486d754",
      "metadata": {
        "id": "283367c5-4134-4378-bb27-829b0486d754"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5), (0.5))\n",
        "])\n",
        "\n",
        "# Loading the data\n",
        "# TODO -- apply transfors to normalize and do data augmentation\n",
        "# TODO -- split train loader for having training / validation set\n",
        "train_dataset = MNIST_HARDBATCH_TRIPLET(    \n",
        "    train = True, \n",
        "    root = \"./data\",\n",
        "    transform=transform,\n",
        "    num_classes = 4, \n",
        "    images_per_class = 10,\n",
        "    custom_len = 32\n",
        ")\n",
        "test_dataset = MNIST_HARDBATCH_TRIPLET(   \n",
        "    train = False, \n",
        "    root = \"./data\",\n",
        "    transform=transform,\n",
        "    num_classes = 4, \n",
        "    images_per_class = 10,\n",
        "    custom_len = 32\n",
        ")\n",
        "\n",
        "# Data loaders for accessing the data\n",
        "batch_size = 1 # This time we have to use batch size of 1\n",
        "num_workers = 2\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, \n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory = True,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True, \n",
        "    num_workers = num_workers, \n",
        "    pin_memory = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dc9b45c-e5a4-4a8a-bbd6-f338ae155c8c",
      "metadata": {
        "id": "5dc9b45c-e5a4-4a8a-bbd6-f338ae155c8c"
      },
      "source": [
        "Check if dataloaders work well, because we're handling batches in non-pytorch way:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c77af17-40e2-4582-8b6f-176a29a21610",
      "metadata": {
        "id": "0c77af17-40e2-4582-8b6f-176a29a21610"
      },
      "outputs": [],
      "source": [
        "for batch in test_loader:    \n",
        "    MNIST_HARDBATCH_TRIPLET.display_batch(batch)\n",
        "    \n",
        "    # Only show one iteration of the dataloader\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64f272b8-b501-49c2-8aa7-77a7abe85614",
      "metadata": {
        "id": "64f272b8-b501-49c2-8aa7-77a7abe85614"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c29de921-e5d9-4319-bf01-1f65e651d734",
      "metadata": {
        "id": "c29de921-e5d9-4319-bf01-1f65e651d734"
      },
      "outputs": [],
      "source": [
        "# Set the training parameters\n",
        "parameters = dict()\n",
        "parameters[\"lr\"] = 0.001\n",
        "parameters[\"momentum\"] = 0.9\n",
        "parameters[\"criterion\"] = triplet_loss_batch_hard\n",
        "parameters[\"epochs\"] = 10\n",
        "\n",
        "# We save the model in different paths wether we're running\n",
        "# local or in cloud\n",
        "save_model_path = None\n",
        "if running_env == \"local\": save_model_path = \"./saved_models\"\n",
        "if running_env == \"cloud\": save_model_path = \"drive/MyDrive/ml/Modelos/MNIST-ReID/saved_models\"\n",
        "\n",
        "# Network we are going to train\n",
        "net = LuNet()\n",
        "\n",
        "# Specifying the trainning logger\n",
        "logger = TripletLogger(\n",
        "    iterations = 200 * batch_size, # TODO -- document why multiplying batch_size \n",
        "    loss_func = triplet_loss_batch_hard, \n",
        "    net = net,\n",
        "    training_perc = 0.2, \n",
        "    validation_perc = 1.0\n",
        ")\n",
        "\n",
        "# If running in cloud, show tensorboard inline\n",
        "if running_env == \"cloud\":\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir runs\n",
        "\n",
        "# Train process\n",
        "core.batch_hard_train(\n",
        "    net, \n",
        "    save_model_path, \n",
        "    parameters, \n",
        "    train_loader = train_loader, \n",
        "    validation_loader = test_loader, # TODO -- we are using test dataset as validation dataset \n",
        "    name = \"LuNet\", \n",
        "    logger = logger,\n",
        "    snapshot_iterations = 10_000 \n",
        ")\n",
        "\n",
        "# Close the logger when we are done with the training\n",
        "logger.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aadebb34-b01d-4a6e-831f-7de00f182789",
      "metadata": {
        "id": "aadebb34-b01d-4a6e-831f-7de00f182789"
      },
      "source": [
        "# Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd1957da-c16c-4f92-a9cd-51e8ba5831f6",
      "metadata": {
        "id": "fd1957da-c16c-4f92-a9cd-51e8ba5831f6"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9ba21cf-ebab-4620-8183-50d8fa89ecb6",
      "metadata": {
        "id": "a9ba21cf-ebab-4620-8183-50d8fa89ecb6"
      },
      "source": [
        "# Some conclusions and thoughts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7b36f9e-96e2-48fb-bdb3-ac1daeb9befd",
      "metadata": {
        "id": "c7b36f9e-96e2-48fb-bdb3-ac1daeb9befd"
      },
      "source": [
        "- Base model with no hyperparameter tuning, and just three fc layers with batch normalization reaches ~97% acc on test set\n",
        "    - Trained with 0.001 lr\n",
        "    - Trained with 0.01 lr converges much faster and also reaches 97% acc on test set\n",
        "- Baseline LuNet model with no hyperparameter tuning also reaches ~96% acc\n",
        "    - Baseline Lunet means we are using the LuNet architecture but cross entropy loss and classic training loop \n",
        "    - For baseline, the size of output layer has to be 10, in our actual model we use higher value for specifying embedding space dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e513ae59-1a3e-4634-beea-65476c09ec51",
      "metadata": {
        "id": "e513ae59-1a3e-4634-beea-65476c09ec51"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Notebook.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}