{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23ef89e1",
    "tags": []
   },
   "source": [
    "# Parámetros globales del *Notebook*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "c8cc3fb4"
   },
   "outputs": [],
   "source": [
    "# Para definir los path\n",
    "import os\n",
    "\n",
    "# Define si estamos ejecutando el Notebook en nuestro \n",
    "# ordenador (\"local\") o en Google Colab (\"remote\")\n",
    "RUNNING_ENV = \"local\"\n",
    "\n",
    "# Path que vamos a usar como base para el resto de paths\n",
    "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "# Directorio en el que guardamos los scripts de python que usamos \n",
    "# como libreria propia\n",
    "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
    "\n",
    "# Directorio en el que guardamos los datos de entrenamiento y test\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
    "\n",
    "# Numero de procesos que queremos usar\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Batch size que queremos usar para entrenamiento offline\n",
    "DATALOADER_BACH_SIZE = 32\n",
    "\n",
    "# Bath size que queremos usar para entrenamiento online\n",
    "ONLINE_BATCH_SIZE = 2**10\n",
    "\n",
    "# Tamaño del conjunto de triples aleatorios\n",
    "# Tiene que ser un multiplo de DATALOADER_BACH_SIZE para que\n",
    "# la red tome correctamente los batches que espera\n",
    "RANDOM_TRIPLETS_DATA_SIZE = DATALOADER_BACH_SIZE * 15\n",
    "\n",
    "# Numero de epocas por las que queremos entrenar\n",
    "TRAINING_EPOCHS = 1\n",
    "\n",
    "# Margen para la funcion de perdida\n",
    "MARGIN = 0.1\n",
    "\n",
    "# Dimension del embedding que calculamos\n",
    "EMBEDDING_DIMENSION = 2\n",
    "\n",
    "# Numero de vecinos a usar en la adaptacion a clasificador\n",
    "NUMBER_NEIGHBOURS = 3\n",
    "\n",
    "# Controla si queremos evitar toda la parte del entrenamiento\n",
    "# con triples aleatorios\n",
    "# TODO -- antes de entregar poner a False\n",
    "SKIP_RANDOM_TRIPLETS_TRAINING = True\n",
    "\n",
    "# Controla si queremos entrenar el modelo y usarlo o \n",
    "# no entrenar nada y cargar el modelo entrenado de disco\n",
    "# Cuando es False, entrenamos y ademas actualizamos el modelo \n",
    "# en disco\n",
    "# TODO -- poner esto a False\n",
    "USE_CACHED_MODEL = True\n",
    "MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c155001b"
   },
   "source": [
    "# Autorización si estamos usando Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6784bd8",
    "outputId": "d3e8a373-259c-47a3-e421-0328c24549f9"
   },
   "outputs": [],
   "source": [
    "if RUNNING_ENV == \"remote\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67d9d57d"
   },
   "source": [
    "# Importando los módulos que vamos a usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "d2c35e87"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Para poder usar ResNet18 preentrenado\n",
    "import torchvision.models as models \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import functools\n",
    "\n",
    "# Todas las piezas concretas que usamos de sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n",
    "\n",
    "# Cargamos en el Notebook todos los ficheros .py que definen nuestra propia libreria\n",
    "# Usamos esta libreria para escribir el codigo base necesario para llevar a cabo ciertas\n",
    "# tareas del notebook (como el bucle de entrenamiento) que no tienen interes mostrar\n",
    "# en este notebook\n",
    "!cp -r \"$LIB_PATH\"/* .\n",
    "\n",
    "# Ahora que hemos cargado estos ficheros en el Notebook, importamos lo necesario\n",
    "# de nuestra propia libreria\n",
    "import core\n",
    "import time\n",
    "import copy\n",
    "import board\n",
    "import filesystem\n",
    "from train_loggers import ClassificationLogger, SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger\n",
    "from models.resnet import *\n",
    "from visualizations import *\n",
    "from custom_loss import triplet_loss_batch_hard\n",
    "from tqdm.notebook import tqdm\n",
    "from core import train_model_offline, train_model_online\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97c7e921"
   },
   "source": [
    "# Funciones comunes que vamos a usar en el notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "08762464"
   },
   "outputs": [],
   "source": [
    "def show_learning_curve(training_history: dict):\n",
    "    # Tomamos las dos funciones de perdida\n",
    "    loss = training_history['loss']\n",
    "    val_loss = training_history['val_loss']\n",
    "    \n",
    "    # Mostramos las graficas\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eea5f6b"
   },
   "source": [
    "# Carga del conjunto de datos\n",
    "\n",
    "- Cargamos los datos de entrenamiento y test\n",
    "- Además, separamos train en train y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "452699e8"
   },
   "outputs": [],
   "source": [
    "# Transformaciones que queremos aplicar al cargar los datos\n",
    "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
    "])\n",
    "\n",
    "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
    "# preparado para descargar\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Separamos train en train y validacion\n",
    "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
    "\n",
    "# Data loaders para acceder a los datos\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = DATALOADER_BACH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = DATALOADER_BACH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size = DATALOADER_BACH_SIZE,\n",
    "  shuffle = True,\n",
    "  num_workers = NUM_WORKERS,\n",
    "  pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c057c3d"
   },
   "source": [
    "# Definiendo las clases con las que vamos a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "b0de0753"
   },
   "outputs": [],
   "source": [
    "# Clases con las que vamos a trabajar\n",
    "# Esta lista especifica la relacion numero -> nombre de la forma\n",
    "# classes[numero] = nombre\n",
    "classes = (\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d74819e",
    "tags": []
   },
   "source": [
    "# Análisis Exploratorio de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73ebca17"
   },
   "source": [
    "Mostramos algunas imágenes con sus clases para asegurar que hemos cargado correctamente las imágenes del conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6913658d",
    "outputId": "50342115-f90e-45ff-d2ea-de97540af996"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase obtenida es: Pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase obtenida es: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase obtenida es: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase obtenida es: Ankle boot\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARjklEQVR4nO3da4zUVZrH8d8DKM1NLqLQIshI1HAxi4jEuF7QiROHxOBEnIjJBrNEfDEmM8m+wLgxo9FNzGZn1n01sSfqMMoymYAoGt0ZNbqsiRERWAFhBLTFbrksIghyEehnX3RhWu3/c9q66/l+kk5311On6vCHH/9/1alzjrm7APzw9Wt0BwDUB2EHMkHYgUwQdiAThB3IxIB6PpmZ8dY/UGPubr3dXtGZ3cxuMrO/mdl2M7u3kscCUFtW7ji7mfWX9L6kGyV1SHpb0nx3fy9ow5kdqLFanNlnSdru7h+4+5eS/iRpbgWPB6CGKgn7OEkf9/i9o3Tb15jZIjNba2ZrK3guABWq+Rt07t4mqU3iMh5opErO7J2Sxvf4/fzSbQCaUCVhf1vSRWb2IzM7U9LtklZVp1sAqq3sy3h3P2lm90j6i6T+kp5w981V6xmAqip76K2sJ+M1O1BzNflQDYDvD8IOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQibouJY3vn3794vNBatYkG4d+d62trWF98ODBhbWOjo7CGmd2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcywTg7Ql1dXTV77JUrV4b1Z599Nqzv3LkzrB8/frywtnXr1rLbSlJLS0tF7Q8fPlxYGzfuW7uofc3NN99cWHvssccKa5zZgUwQdiAThB3IBGEHMkHYgUwQdiAThB3IBOPsCJn1uiHoV1Lz1RcvXlxYGzAg/uc3b968sD5q1KiwHvV9+PDhYdtU31J/7mPHjoX1ffv2FdaOHj0atn344YcLawcPHiysVRR2M2uXdEjSKUkn3X1mJY8HoHaqcWa/3t2L/5sC0BR4zQ5kotKwu6S/mtk7ZraotzuY2SIzW2tmayt8LgAVqPQy/mp37zSzcyW9bGZb3X11zzu4e5ukNkkyM1YfBBqkojO7u3eWvu+VtFLSrGp0CkD1lR12MxtiZsNO/yzpJ5I2VatjAKqrksv4MZJWlsYyB0j6T3f/r6r0CnWTGk8+efJkWL/qqqvC+m233VZY27FjR9g2Wh9dSo9ln3/++YW1jRs3hm1feOGFsD5lypSwfsMNN4T1YcOGFdaOHDkStn3rrbfCepGyw+7uH0j6u3LbA6gvht6ATBB2IBOEHcgEYQcyQdiBTDDFNXOVbqm8Zs2asL5ly5bC2ogRI8K2qaG3oUOHhvWzzjqrsPbQQw+FbQ8cOBDWly5dGtYfffTRsB4NzZU7tJbCmR3IBGEHMkHYgUwQdiAThB3IBGEHMkHYgUwwzp65/v37h/VTp06F9WgKqyRNmDChsJaaopoyaNCgsH733XcX1mbMmBG2bW9vD+vjx48P6+vXrw/r11xzTWFt1apVYdtycWYHMkHYgUwQdiAThB3IBGEHMkHYgUwQdiATjLP/wKW2XP7yyy8revzZs2eX3Ta1ZHJqS+YTJ06U/fgLFiwI2y5btiysz5kzJ6w///zzYX337t2FNeazA6gIYQcyQdiBTBB2IBOEHcgEYQcyQdiBTDDO/gOXmq+e2pL5rrvuCuvTpk0L6x0dHYW11LrxXV1dYT3VvqWlpbDW2dkZtp00aVJYT81379cvPo9Onjy5sHbo0KGwbbmSZ3Yze8LM9prZph63jTKzl81sW+n7yJr0DkDV9OUy/g+SbvrGbfdKetXdL5L0aul3AE0sGXZ3Xy1p/zdunitpSennJZJuqW63AFRbua/Zx7j7rtLPuyWNKbqjmS2StKjM5wFQJRW/QefubmaFuwO6e5ukNkmK7gegtsodettjZq2SVPq+t3pdAlAL5YZ9laTTcwQXSHquOt0BUCvJy3gzWyZptqTRZtYh6deSHpH0ZzNbKOkjST+vZSdRvtR4b8qePXvCemr99L17iy/6BgyI//mNHBmP6Kbmuy9evLiwNm/evLBtaqx77ty5Yf3OO+8M6zt27AjrtZAMu7vPLyj9uMp9AVBDfFwWyARhBzJB2IFMEHYgE4QdyARTXH8AouWiK10qety4cWE9tTVxNA112LBhYdvPPvssrKemqU6fPr2wtnz58rBt6s913XXXhfWhQ4eG9XXr1oX1WuDMDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJsy9fovHsFJNbQwcOLCwdvz48bDtzJkzw/prr70W1l966aWwfskllxTWUn07duxYWE+JxulTjz1x4sSwnlqi++jRo2E9ev4bb7wxbJvi7r1+8IIzO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmWCc/XvgzDPPDOvRnPUJEyaEbdva2sL6rl27wvoFF1wQ1s8+++zCWmqcffjw4WH9/fffD+uDBg0qrK1atSpsmzJ79uywPnbs2LAerROQOqYpjLMDmSPsQCYIO5AJwg5kgrADmSDsQCYIO5AJ1o3/Hqhk7ffbb789rH/xxRdhffDgwWE9NRbe1dVVWEutC//iiy+G9WiuvCStWLGisLZw4cKwbXt7e1hPraefmi8fbYU9evTosO2+ffvCepHkmd3MnjCzvWa2qcdtD5hZp5ltKH3NKevZAdRNXy7j/yDppl5u/3d3n176iv8LBtBwybC7+2pJ++vQFwA1VMkbdPeY2buly/yRRXcys0VmttbM1lbwXAAqVG7YfydpkqTpknZJ+k3RHd29zd1nunu8siGAmior7O6+x91PuXuXpN9LmlXdbgGotrLCbmatPX79maRNRfcF0ByS4+xmtkzSbEmjzaxD0q8lzTaz6ZJcUruku2vXxeZ3xhlnhPXUmgEnT54M6xdffHFYf/LJJwtrn3zySdh29+7dYf3aa68N66dOnQrr0Z9t//74fd/t27eH9dS87zvuuCOsR2bMmBHWU59POHLkSFhvaWkprE2ZMiVsu3r16rBeJBl2d5/fy82Pl/VsABqGj8sCmSDsQCYIO5AJwg5kgrADmWiqpaTNel0B9ysDBhQPHqT+HP36xf+vpeqVbh9ciaeeeiqsR0NUQ4cODdteeeWVZfXptNRxibaTjpZ6lqStW7eG9dbW1rA+fvz4wlpqem3q32Jq2vGJEyfC+pAhQwprqam9Dz74YFhnKWkgc4QdyARhBzJB2IFMEHYgE4QdyARhBzJR96Wko/HL1Fh5auyyUVLbIl922WVh/frrrw/ra9fGK3pFyxpfeOGFYdvUFNhDhw6F9WgcXYqn/44cWbiamaR4u+e+iKbvRlNMpfQ22f379w/rqc9tHD16tLA2efLksG25OLMDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5CJuo+zVzJ/furUqYW1Sy+9NGybGrNNjZueddZZhbXOzs6w7c6dO8P6pEmTwvrll18e1qN5/qk546lx9NRx+fzzz8N69LmK1Fh36nMVqb/TaM54VJOk48ePh/XUEtqpbZWj+fCpZarLxZkdyARhBzJB2IFMEHYgE4QdyARhBzJB2IFM1H2cPXL//feH9XPPPbewllpbfcOGDWE9tW3y3r17C2vnnXde2PbWW28N64MHDw7rBw4cCOvDhg0rrKXG0VPzrlPj7Oecc05Yj8arU2P0qXH2Tz/9NKxHn29IPXf02QUpPd891fdoDYLUGH65kmd2MxtvZq+Z2XtmttnMflm6fZSZvWxm20rf45UIADRUXy7jT0r6J3efIulKSb8wsymS7pX0qrtfJOnV0u8AmlQy7O6+y93XlX4+JGmLpHGS5kpaUrrbEkm31KiPAKrgO71mN7OJki6T9JakMe6+q1TaLWlMQZtFkhZV0EcAVdDnd+PNbKikFZJ+5e5fe3fDu2e39DrDxd3b3H2mu8+sqKcAKtKnsJvZGeoO+lJ3f6Z08x4zay3VWyUVv10NoOGSl/HWPUfxcUlb3P23PUqrJC2Q9Ejp+3Opxxo4cGC4jW5qKmc0jDN//vywbXt7e1hPbcE7duzYwlo09CWlh7c+/PDDsJ4a2ouGeVJDSKnlnA8fPhzWK1mKOnVcUlsyp9pH03vHjOn1VedXoiWwU48tpYc8o7+X/fv3h23L1ZfX7H8v6R8kbTSzDaXb7lN3yP9sZgslfSTp5zXpIYCqSIbd3d+QVLQCwY+r2x0AtcLHZYFMEHYgE4QdyARhBzJB2IFM1HWK68CBA8Nlk1PL+44YMaKwlhqLnjFjRliPljyW4imwqTHV1Fh0akw3NY4fjdmmpu4eOXIkrKf6NnHixLA+bdq0sB7Ztm1bWI+2PZbiz06kPn+QWko6ddxS9Wga68GDB8O25eLMDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJqySLZS/85OZhU+WGpNduHBhYS019zmaRy+ll0yOpOaEp+Zdp8b4u7q6yn781Dj5qFGjwnpqvHj9+vVh/emnny6sLV26NGw7evTosL5mzZqwHq1hUOnfSeq4psbpo/Zbt24N20Y5kCR377XznNmBTBB2IBOEHcgEYQcyQdiBTBB2IBOEHchEU42z19LUqVPD+qxZs8L6FVdcUVhLjeG3tLSE9dQ4eqoebem8efPmsO3rr78e1t94442wXkupOecff/xxWH/zzTcLa6lx8lQuUp8/SK1xEM1nX758edj2lVdeCeuMswOZI+xAJgg7kAnCDmSCsAOZIOxAJgg7kInkOLuZjZf0R0ljJLmkNnf/DzN7QNJdkv6vdNf73P3FxGM1bJwdyEXROHtfwt4qqdXd15nZMEnvSLpF3fuxH3b3f+trJwg7UHtFYe/L/uy7JO0q/XzIzLZIGlfd7gGote/0mt3MJkq6TNJbpZvuMbN3zewJM+t1bSYzW2Rma81sbWVdBVCJPn823syGSvpvSf/i7s+Y2RhJ+9T9Ov4hdV/q/2PiMbiMB2qs7NfskmRmZ0h6QdJf3P23vdQnSnrB3cMVIwk7UHtlT4Sx7mU2H5e0pWfQS2/cnfYzSZsq7SSA2unLu/FXS/ofSRslnZ5reZ+k+ZKmq/syvl3S3aU386LH4swO1FhFl/HVQtiB2mM+O5A5wg5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJwg5kIrngZJXtk/RRj99Hl25rRs3at2btl0TfylXNvl1QVKjrfPZvPbnZWnef2bAOBJq1b83aL4m+latefeMyHsgEYQcy0eiwtzX4+SPN2rdm7ZdE38pVl7419DU7gPpp9JkdQJ0QdiATDQm7md1kZn8zs+1mdm8j+lDEzNrNbKOZbWj0/nSlPfT2mtmmHreNMrOXzWxb6Xuve+w1qG8PmFln6dhtMLM5DerbeDN7zczeM7PNZvbL0u0NPXZBv+py3Or+mt3M+kt6X9KNkjokvS1pvru/V9eOFDCzdkkz3b3hH8Aws2slHZb0x9Nba5nZv0ra7+6PlP6jHOnui5ukbw/oO27jXaO+FW0zfqcaeOyquf15ORpxZp8labu7f+DuX0r6k6S5DehH03P31ZL2f+PmuZKWlH5eou5/LHVX0Lem4O673H1d6edDkk5vM97QYxf0qy4aEfZxkj7u8XuHmmu/d5f0VzN7x8wWNbozvRjTY5ut3ZLGNLIzvUhu411P39hmvGmOXTnbn1eKN+i+7Wp3nyHpp5J+UbpcbUre/RqsmcZOfydpkrr3ANwl6TeN7Expm/EVkn7l7p/3rDXy2PXSr7oct0aEvVPS+B6/n1+6rSm4e2fp+15JK9X9sqOZ7Dm9g27p+94G9+cr7r7H3U+5e5ek36uBx660zfgKSUvd/ZnSzQ0/dr31q17HrRFhf1vSRWb2IzM7U9LtklY1oB/fYmZDSm+cyMyGSPqJmm8r6lWSFpR+XiDpuQb25WuaZRvvom3G1eBj1/Dtz9297l+S5qj7Hfkdkv65EX0o6NeFkv639LW50X2TtEzdl3Un1P3exkJJZ0t6VdI2Sa9IGtVEfXtK3Vt7v6vuYLU2qG9Xq/sS/V1JG0pfcxp97IJ+1eW48XFZIBO8QQdkgrADmSDsQCYIO5AJwg5kgrADmSDsQCb+H3i0rwu6Zl+/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La clase obtenida es: Trouser\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgs_to_show = 5\n",
    "\n",
    "for _ in range(imgs_to_show):\n",
    "\n",
    "    # Cargamos un batch de imagenes\n",
    "    images, images_classes = next(iter(train_loader))\n",
    "\n",
    "    # Nos quedamos con la primera imagen del batch\n",
    "    img, img_class = images[0], images_classes[0]\n",
    "\n",
    "    # Mostramos alguna informacion de la imagen\n",
    "    print(f\"La clase obtenida es: {classes[img_class]}\")\n",
    "\n",
    "    # Re-escalamos y mostramos la imagen\n",
    "    img = img.reshape((28, 28))\n",
    "    show_img(img, color_format_range = (-1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64b64e5c"
   },
   "source": [
    "Mostramos ahora unas cuantas imágenes de forma simultánea:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "983f7e35"
   },
   "source": [
    "Mostramos ahora los tamaños del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "762ead5e",
    "outputId": "6bbc6a80-47f8-42ed-837e-e3a38befd9f4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenemos 48000 imágenes de entrenamiento\n",
      "Tenemos 10000 imágenes de test\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tenemos {len(train_dataset)} imágenes de entrenamiento\")\n",
    "print(f\"Tenemos {len(test_dataset)} imágenes de test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c4ab9cd"
   },
   "source": [
    "# Definiendo el modelo base\n",
    "\n",
    "- A continuación, definimos el modelo que vamos a usasr como base para nuestra red siamesa\n",
    "- Usaremos el modelo pre-entrenado ResNet18, pre-entrenado en ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8d3a00ed"
   },
   "outputs": [],
   "source": [
    "class ResNet18(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        # Tomamos el modelo pre-entrenado ResNet18\n",
    "        self.pretrained = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Cambiamos la primera convolucion para que en vez\n",
    "        # de tres canales acepte un canal para las imagenes\n",
    "        # de entrada\n",
    "        self.pretrained.conv1 = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # Cambiamos la ultima capa fc Linear(in_features=512, out_features=1000, bias=True)\n",
    "        # para calcular un embedding de dimension mucho menor\n",
    "        # TODO -- comentar en la memoria el cambio de ERROR que hacer esto nos ha supuesto\n",
    "        self.pretrained.fc = nn.Linear(in_features=512, out_features=EMBEDDING_DIMENSION, bias=True)\n",
    "        \n",
    "        # Por defecto siempre realizamos la permutacion del tensor de entrada\n",
    "        self.should_permute = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # Tenemos como entrada tensores (1, DATALOADER_BACH_SIZE, 28, 28) y \n",
    "        # queremos tensores (DATALOADER_BACH_SIZE, 1, 28, 28) para poder trabajar\n",
    "        # con la red pre-entrenada\n",
    "        # Usamos permute en vez de reshape porque queremos que tambien funcione al\n",
    "        # realizar inferencia con distintos tamaños de minibatch (ie. 1)\n",
    "        if self.should_permute is True:\n",
    "            x = torch.permute(x, (1, 0, 2, 3))\n",
    "\n",
    "        # Usamos directamente la red pre-entrenada para hacer el forward\n",
    "        x = self.pretrained.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def set_permute(self, should_permute: bool):\n",
    "        self.should_permute = should_permute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40bdcf53"
   },
   "source": [
    "# Generación de triples\n",
    "\n",
    "- Para entrenar la red siamesa, necesitamos dar triples con los que computar el *triplet loss*\n",
    "- Por ello, es necesaria una fase previa de *triplets mining*\n",
    "- En todos los casos, crearemos *Datasets* de *Pytorch* para representar la creación de los triples\n",
    "- Hacemos esto basándonos el la [documentación oficial de Pytorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "142db1bf"
   },
   "source": [
    "## Generación de triples aleatorios\n",
    "\n",
    "- Es la forma más sencilla y directa para generar triples\n",
    "- Usaremos esta generación como baseline para más tarde realizar comparaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "ef0dbb9f",
    "outputId": "f7735afe-8254-4494-d138-059d824b9dfe"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "class RandomTriplets(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset en el que los elementos son triples obtenidos de forma aleatoria\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_data: Dataset, custom_len: int, transform = None):\n",
    "        self.base_data = base_data\n",
    "        self.custom_len = custom_len\n",
    "        self.transform = transform\n",
    "        self.random_sampler = RandomSampler(self.base_data, replacement=True, num_samples=1, generator=None)\n",
    "        \n",
    "        # Por motivos de eficiencia, pre-computamos una lista de listas, de forma\n",
    "        # que tengamos disponibles las listas con las posiciones de cada clase por\n",
    "        # separado.\n",
    "        self.posiciones_clases = self.__precompute_list_of_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Devolvemos el tamaño del dataset\n",
    "        Como estamos generando triples aleatorios, devolvemos el tamaño definido\n",
    "        por parametro\n",
    "        \"\"\"\n",
    "        return self.custom_len\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Funcion que es llamada cuando se hace dataset[idx]\n",
    "        En vez de devolver una imagen (que es lo comun en esta clase dataset), \n",
    "        devolvemos un triple (anchor, positive, negative) aleatorio\n",
    "        \"\"\"\n",
    "\n",
    "        # Hacemos esto por temas de eficiencia\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Tomamos una imagen aleatoria que sera el ancla\n",
    "        anchor, anchor_class = self.base_data[next(iter(self.random_sampler))]\n",
    "\n",
    "        # Tomamos una imagen de la misma clase, que sera la positiva, de forma aleatoria\n",
    "        random_index = np.random.choice(self.posiciones_clases[anchor_class])\n",
    "        positive, positive_class = self.base_data[random_index]\n",
    "\n",
    "        # Tomamos una imagen de otra clase, que sera la negativa\n",
    "        # Empiezo tomando una clase que no sea la del anchor\n",
    "        posible_classes = list(range(10))\n",
    "        posible_classes.remove(anchor_class)\n",
    "        negative_class = np.random.choice(posible_classes)\n",
    "\n",
    "        # Ahora tomamos un indice aleatorio de esta clase negativa\n",
    "        random_index = np.random.choice(self.posiciones_clases[negative_class])\n",
    "        negative, negative_class = self.base_data[random_index]\n",
    "        \n",
    "        # Generamos ahora el triple\n",
    "        triplet = [anchor, positive, negative]\n",
    "\n",
    "        # Aplicamos la transformacion dada al dataset al ejemplo que devolvemos\n",
    "        if self.transform:\n",
    "            triplet = [self.transform(np.array(img)) for img in triplet]\n",
    "\n",
    "        return triplet\n",
    "\n",
    "    def __precompute_list_of_classes(self) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Calcula la lista con las listas de posiciones de cada clase por separado\n",
    "        \"\"\"\n",
    "        # Inicializamos la lista de listas\n",
    "        posiciones_clases = [[] for _ in range(10)]\n",
    "\n",
    "        # Recorremos el dataset y colocamos los indices donde corresponde\n",
    "        for idx, element in enumerate(self.base_data):\n",
    "            _, img_class = element\n",
    "            posiciones_clases[img_class].append(idx)\n",
    "\n",
    "        return posiciones_clases\n",
    "\n",
    "    \n",
    "class CustomReshape(object):\n",
    "    \"\"\"Pasamos la imagen de (28, 1, 28) a (28, 28)\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = image.reshape(28, 28)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "   \n",
    "    # Antes de modificar la base de datos para convertirla a triples\n",
    "    # la guardamos, porque mas adelante nos hara falta\n",
    "    old_train_dataset = train_dataset\n",
    "    old_test_dataset = test_dataset\n",
    "\n",
    "    # Necesitamos hacer reshape de las imagenes para que\n",
    "    # sean (28, 28) y no (28, 1, 28)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # Hacemos reshape de las imagenes para\n",
    "        # que sean tensores (28, 28)\n",
    "        CustomReshape(),\n",
    "    ])\n",
    "\n",
    "    # Generamos los triples aleatorios para training\n",
    "    random_triplets_train = RandomTriplets(\n",
    "        base_data = train_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_train,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Generamos los triples aleatorios para validacion\n",
    "    random_triplets_validation = RandomTriplets(\n",
    "        base_data = validation_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_validation,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Generamos los triples aleatorios para test\n",
    "    random_triplets_test = RandomTriplets(\n",
    "        base_data = test_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_test,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Visualizamos algunos triples aleatorios para comprobar el funcionamiento\n",
    "    custom_triplet = random_triplets_train[2]\n",
    "    for i in custom_triplet :\n",
    "        show_img(i, color_format_range = (-1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la función de pérdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor: torch.Tensor, positive: torch.Tensor, negative: torch.Tensor) -> torch.Tensor:       \n",
    "       \n",
    "        distance_positive = self.euclidean_distance(anchor, positive)\n",
    "        distance_negative = self.euclidean_distance(anchor, negative)\n",
    "\n",
    "        # Usamos Relu para que el error sea cero cuando la resta de las distancias\n",
    "        # este por debajo del margen. Si esta por encima del margen, devolvemos la\n",
    "        # identidad de dicho error. Es decir, aplicamos Relu a la formula que \n",
    "        # tenemos debajo\n",
    "        loss = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def euclidean_distance(self, first: torch.Tensor, second: torch.Tensor) -> float:\n",
    "        return ((first - second) * (first - second)).sum()\n",
    "    \n",
    "class TripletLossCustom(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLossCustom, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.base_loss = TripletLoss(self.margin)\n",
    "    \n",
    "    def forward(self, batch: torch.Tensor) -> torch.Tensor:\n",
    "        losses = torch.tensor(\n",
    "            [self.base_loss(current[0], current[1], current[2]) for current in batch], \n",
    "            requires_grad=True\n",
    "        )\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a17b1e1"
   },
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (pretrained): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "\n",
    "# TODO -- fijar bien los parametros\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
    "parameters[\"lr\"] = 0.001\n",
    "parameters[\"criterion\"] = TripletLossCustom(MARGIN)\n",
    "\n",
    "# Definimos el logger que queremos para el entrenamiento\n",
    "logger = TripletLoggerOffline(\n",
    "    net = net,\n",
    "    iterations = 10 * DATALOADER_BACH_SIZE,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    training_history = train_model_offline(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetwork\",\n",
    "        logger = logger,\n",
    "        snapshot_iterations = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc92ce37"
   },
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "- Mostramos algunas métricas fundamentales sobre el conjunto de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2242eedc"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: core.test_model(net, test_loader, parameters[\"criterion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c946904"
   },
   "source": [
    "# Adaptación del modelo para usarlo como clasificador\n",
    "\n",
    "- Nuestro modelo genera un *embedding*\n",
    "- Adaptamos el modelo para que, a partir de dicho embedding, podamos usarlo como un clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3d33aa9d"
   },
   "outputs": [],
   "source": [
    "class EmbeddingToClassifier:\n",
    "    \"\"\"\n",
    "    Clase que toma un modelo que calcula embeddings y lo convierte\n",
    "    a un modelo que sirve para una tarea de clasificacion\n",
    "    \n",
    "    Usamos K-NN para el modelo de clasificacion\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedder: nn.Module, k: int, data_loader):\n",
    "        \n",
    "        # El modelo que calcula los embeddings\n",
    "        self.embedder = embedder\n",
    "         \n",
    "        # Dataloader que representa el dataset que usamos para k-nn\n",
    "        self.data_loader = data_loader\n",
    "        \n",
    "        # Tomamos el dispositivo en el que esta el modelo y los\n",
    "        # datos, porque nos va a hacer falta durante todo el codigo\n",
    "        self.device = core.get_device()\n",
    "        \n",
    "        # Calculamos todos los embeddings de los puntos\n",
    "        self.dataset_embedded = self.__calculate_embedding()\n",
    "        \n",
    "        # Modelo de clasificacion k-nn\n",
    "        self.k = k\n",
    "        self.knn = self.__fit_knn()\n",
    "                \n",
    "    def predict_proba(self, img, batch_mode: bool = False) -> int:\n",
    "        \n",
    "        # Ponemos la red en modo evaluacion\n",
    "        self.embedder.eval()\n",
    "        \n",
    "        # Tenemos una unica imagen, lo que queremos es\n",
    "        # tener un batch de una imagen para que la red\n",
    "        # pueda trabajar con ello\n",
    "        single_img_batch = torch.Tensor(img)\n",
    "        \n",
    "        # Calculamos el embedding de la imagen\n",
    "        img_embedding = None\n",
    "        if batch_mode is False:\n",
    "            img_embedding = self.embedder(single_img_batch[None, ...].to(self.device)) \n",
    "        else:\n",
    "            img_embedding = self.embedder(single_img_batch.to(self.device)) \n",
    "        \n",
    "        # Pasamos el embedding a cpu que es donde esta \n",
    "        # el modelo knn de scikit learn\n",
    "        img_embedding = img_embedding.cpu().detach().numpy() \n",
    "        \n",
    "        # Antes de salir de la funcion volvemos a poner\n",
    "        # a la red en modo entrenamiento\n",
    "        self.embedder.train()\n",
    "        \n",
    "        # Usamos dicho embedding para clasificar con knn\n",
    "        return self.knn.predict(img_embedding)\n",
    "    \n",
    "    def predict(self, img, batch_mode: bool = False) -> int:\n",
    "        \n",
    "        # Ponemos la red en modo evaluacion\n",
    "        self.embedder.eval()\n",
    "        \n",
    "        # Tenemos una unica imagen, lo que queremos es\n",
    "        # tener un batch de una imagen para que la red\n",
    "        # pueda trabajar con ello\n",
    "        single_img_batch = torch.Tensor(img)\n",
    "        \n",
    "        # Calculamos el embedding de la imagen\n",
    "        img_embedding = None\n",
    "        if batch_mode is False:\n",
    "            img_embedding = self.embedder(single_img_batch[None, ...].to(self.device)) \n",
    "        else:\n",
    "            img_embedding = self.embedder(single_img_batch.to(self.device)) \n",
    "        \n",
    "        # Pasamos el embedding a cpu que es donde esta \n",
    "        # el modelo knn de scikit learn\n",
    "        img_embedding = img_embedding.cpu().detach().numpy() \n",
    "        \n",
    "        # Antes de salir de la funcion volvemos a poner\n",
    "        # a la red en modo entrenamiento\n",
    "        self.embedder.train()\n",
    "        \n",
    "        # Usamos dicho embedding para clasificar con knn\n",
    "        return self.knn.predict(img_embedding)\n",
    "\n",
    "                    \n",
    "    def predict_using_embedding(self, embedding: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Realizamos la prediccion, pero en vez de usando la imagen\n",
    "        pasamos directamente el embedding de la imagen (en ocasiones\n",
    "        podemos mejorar el rendimiento pre-computando el embedding de\n",
    "        todo un conjunto de imagenes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Usamos dicho embedding para clasificar con knn\n",
    "        return self.knn.predict(embedding)\n",
    "    \n",
    "    def predict_proba_using_embedding(self, embedding: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Realizamos la prediccion, pero en vez de usando la imagen\n",
    "        pasamos directamente el embedding de la imagen (en ocasiones\n",
    "        podemos mejorar el rendimiento pre-computando el embedding de\n",
    "        todo un conjunto de imagenes)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Usamos dicho embedding para clasificar con knn\n",
    "        return self.knn.predict_proba(embedding)\n",
    "        \n",
    "    def __calculate_embedding(self):\n",
    "        \"\"\"Dado el conjunto de imagenes con sus etiquetas, calculamos\n",
    "        el conjunto de embedding con sus etiquetas\"\"\"\n",
    "        \n",
    "        embedded_imgs = []\n",
    "        labels = []\n",
    "\n",
    "        # Por motivos que desconocemos, ahora los tensores vienen\n",
    "        # en el formato que espera la red, asi que no tenemos que\n",
    "        # realizar la permutacion del tensor\n",
    "        self.embedder.set_permute(False)\n",
    "        \n",
    "        for img, img_class in self.data_loader:\n",
    "            \n",
    "            # TODO -- esto hay que borrarlo\n",
    "            if np.random.rand() < 0.01:\n",
    "                break\n",
    "            \n",
    "            # Calculamos el embedding de la imagen\n",
    "            img_embedding = self.embedder(img.to(self.device))\n",
    "            \n",
    "            # Añadimos el embedding asociado a la etiqueta\n",
    "            embedded_imgs.append(img_embedding)\n",
    "            labels.append(img_class)\n",
    "        \n",
    "        # Antes de devolver los datos, volvemos a colocar la opcion\n",
    "        # de que permute los tensores\n",
    "        self.embedder.set_permute(True)\n",
    "\n",
    "        return embedded_imgs, labels\n",
    "\n",
    "    def __fit_knn(self):\n",
    "        \n",
    "        # Tomamos los datos en el formato que espera sklearn\n",
    "        # para realizar el fit\n",
    "        x, y = self.prepare_data_for_sklearn()\n",
    "        \n",
    "        knn = KNeighborsClassifier(n_neighbors = self.k)\n",
    "        knn.fit(x, y)\n",
    "        return knn\n",
    "\n",
    "    def scatter_plot(self):\n",
    "        \"\"\"\n",
    "        Hacemos un scatter plot del embedding obtenido\n",
    "        \"\"\"\n",
    "        \n",
    "        # Solo hacemos este plot cuando la dimension del \n",
    "        # embedding es 2\n",
    "        if EMBEDDING_DIMENSION != 2:\n",
    "            return\n",
    "        \n",
    "        # Tomamos los datos en el formato adecuado para hacer el plot\n",
    "        x, y = self.prepare_data_for_sklearn()\n",
    "        \n",
    "        # Los ejes x,y son los datos de nuestro vector x\n",
    "        # El color de los puntos lo dan las etiquetas almacenadas en y\n",
    "        plt.scatter(x = x[:, 0], y = x[:, 1], c = y)\n",
    "        plt.show()\n",
    "        \n",
    "    def prepare_data_for_sklearn(self): \n",
    "        \"\"\"\n",
    "        Tomamos las imagenes y las etiquetas, y las devolvemos en un\n",
    "        formato adecuado para sklearn y matplotlib. Esto es:\n",
    "            - Pasar los datos a memoria RAM\n",
    "            - Aplanar los datos (tenemos los datos agrupados en minibatches)\n",
    "        \"\"\"\n",
    "\n",
    "        # Separamos los datos segun espera sklearn\n",
    "        x = self.dataset_embedded[0]\n",
    "        y = self.dataset_embedded[1]\n",
    "\n",
    "        # Pasamos de una lista de sublistas (por los minibatches)\n",
    "        # a una lista. Tomamos la idea de:\n",
    "        # https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n",
    "        x = [item for sublist in x for item in sublist]\n",
    "        y = [item for sublist in y for item in sublist]\n",
    "\n",
    "        # Forzamos a usar la memoria RAM (podrian estar los datos\n",
    "        # en memoria GPU) \n",
    "        x = np.array([element.cpu().detach().numpy() for element in x])\n",
    "        y = np.array([element.cpu().detach().numpy() for element in y])\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la adaptación al clasificador, necesitamos tomar las imágenes junto a sus clases. Antes teníamos conjuntos de triples sin clases, ahora recuperamos la información original necesaria para la adaptación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    # Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
    "    # preparado para descargar\n",
    "    train_dataset = old_train_dataset\n",
    "    test_dataset = old_test_dataset\n",
    "\n",
    "    # Data loaders para acceder a los datos\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "\n",
    "    classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    # Hacemos esto y no `in test_dataset[:max_iterations]`\n",
    "    # para no tener que tomar todo el dataset y quedarnos con\n",
    "    # una parte de el, que es un proceso mucho mas lento que usar\n",
    "    # el iterador que da `in test_dataset` y parar con el contador\n",
    "    counter = 0\n",
    "    max_iterations = 20\n",
    "\n",
    "    for img, img_class in test_dataset:\n",
    "        predicted_class = classifier.predict(img)\n",
    "        print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
    "\n",
    "        # Actualizamos el contador\n",
    "        counter += 1\n",
    "        if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot del embedding\n",
    "\n",
    "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
    "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f644bbed"
   },
   "source": [
    "## Evaluación del clasificador obtenido\n",
    "\n",
    "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "030af6e5"
   },
   "outputs": [],
   "source": [
    "def calculate_roc_auc(true_labels_prob: np.array, predicted_labels_prob: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el area bajo la curva ROC, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \n",
    "    Las listas de etiquetas deben ser etiquetas probabilisticas\n",
    "    \"\"\"\n",
    "    return roc_auc_score(true_labels_prob, predicted_labels_prob, multi_class = \"ovo\")\n",
    "    \n",
    "def calculate_accuracy(true_labels: np.array, predicted_labels: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el accuracy, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \"\"\"\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "def calculate_silhouette(x, y):\n",
    "    \"\"\"Calcula el indice de silhouette para el embedding calculado por el modelo\"\"\"\n",
    "    return silhouette_score(x, y)\n",
    "\n",
    "def evaluate_model(model, train_loader, test_loader) -> dict:\n",
    "    \"\"\"\n",
    "    Evalua, usando distintas metricas, el modelo que hemos entrenado\n",
    "    Tambien evaluamos el embedding obtenido, no solo el clasificador\n",
    "    \n",
    "    Devuelve un diccionario con todas las metricas calculadas con el modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario en el que vamos a almacenar todas las metricas\n",
    "    metrics = dict()\n",
    "    \n",
    "    # Tomamos los arrays en formato adecuado para calcular ciertas metricas\n",
    "    x_train, y_train = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader).prepare_data_for_sklearn()\n",
    "    x_test, y_test = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = test_loader).prepare_data_for_sklearn()\n",
    "    \n",
    "    # Empezamos usando el modelo para realizar las predicciones+\n",
    "    # Usamos predicciones probabilisticas pues estas son fundamentales para\n",
    "    # la metrica roc auc\n",
    "    train_predicted_labels_prob = model.predict_proba_using_embedding(x_train)\n",
    "    test_predicted_labels_prob = model.predict_proba_using_embedding(x_test)\n",
    "\n",
    "    # Tomamos ahora las etiqeutas sin probabilidad\n",
    "    train_predicted_labels= model.predict_using_embedding(x_train)\n",
    "    test_predicted_labels = model.predict_using_embedding(x_test)\n",
    "    \n",
    "    # Tomamos las metricas de accuracy\n",
    "    metrics[\"train_acc\"] = calculate_accuracy(y_train, train_predicted_labels)\n",
    "    metrics[\"test_acc\"] = calculate_accuracy(y_test, test_predicted_labels)\n",
    "    \n",
    "    # Tomamos las areas bajo la curva ROC\n",
    "    metrics[\"train_roc_auc\"] = calculate_roc_auc(y_train, train_predicted_labels_prob)\n",
    "    metrics[\"test_roc_auc\"] = calculate_roc_auc(y_test, test_predicted_labels_prob)\n",
    "    \n",
    "    # Tomamos el indice de silhouette\n",
    "    metrics[\"train_silhouette\"] = calculate_silhouette(x_train, y_train)\n",
    "    metrics[\"test_silhouette\"] = calculate_silhouette(x_test, y_test)\n",
    "\n",
    "    # Devolvemos las metricas en formato diccionario, que nos va a ser comodo para\n",
    "    # pasarlas a tablas y para mostrar muchas metricas simultaneamente\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    classifier.embedder.set_permute(False)\n",
    "\n",
    "    metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "    pprint(metrics)\n",
    "\n",
    "    classifier.embedder.set_permute(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definición de la función de pérdida online\n",
    "\n",
    "- Comenzamos el trabajo para adaptar el entrenamiento al uso de triples difíciles dentro de un minibatch, de forma online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copiamos esto de https://stackoverflow.com/a/22279947\n",
    "# Lo necesitamos para saltarnos el elemento de una lista de\n",
    "# forma eficiente\n",
    "import itertools as it\n",
    "def skip_i(iterable, i):\n",
    "    itr = iter(iterable)\n",
    "    return it.chain(it.islice(itr, 0, i), it.islice(itr, 1, None))\n",
    "\n",
    "\n",
    "class OnlineTripletLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, margin=1.0):\n",
    "        super(OnlineTripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.base_loss = TripletLoss(self.margin)\n",
    "        \n",
    "        # Pre-computamos una lista de listas en la que accedemos a los \n",
    "        # elementos de la forma list[label][posicion]\n",
    "        # Con esto nos evitamos tener que realizar la separacion en positivos\n",
    "        # y negativos repetitivamente\n",
    "        # \n",
    "        # Notar que el pre-computo debe realizarse por cada llamada a forward,\n",
    "        # con el minibatch correspondiente. Por tanto, nos beneficia usar minibatches\n",
    "        # grandes\n",
    "        self.list_of_classes = None\n",
    "        \n",
    "        # Si queremos usar self.list_of_classes para calcular todos los \n",
    "        # negativos de una clase, necesitamos dos for que vamos a repetir\n",
    "        # demasiadas veces\n",
    "        self.list_of_negatives = None\n",
    "        \n",
    "    def forward(self, embeddings: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:        \n",
    "        \n",
    "        loss = 0\n",
    "                \n",
    "        # Pre-computamos la separacion en positivos y negativos\n",
    "        self.list_of_classes = self.__precompute_list_of_classes(labels)\n",
    "        \n",
    "        # Pre-computamos la lista de negativos de cada clase\n",
    "        self.list_of_negatives = self.__precompute_negative_class()\n",
    "        \n",
    "        # Iteramos sobre todas los embeddings de las imagenes del dataset\n",
    "        for embedding, img_label in zip(embeddings, labels):\n",
    "\n",
    "            # Calculamos las distancias a positivos y negativos\n",
    "            # Nos aprovechamos de la pre-computacion\n",
    "            positive_distances = [\n",
    "                self.base_loss.euclidean_distance(embedding, embeddings[positive])\n",
    "                for positive in self.list_of_classes[img_label]\n",
    "            ]\n",
    "            \n",
    "            # Ahora nos aprovechamos del segundo pre-computo realizado\n",
    "            negative_distances = [\n",
    "                self.base_loss.euclidean_distance(embedding, embeddings[negative])\n",
    "                for negative in self.list_of_negatives\n",
    "            ]\n",
    "            \n",
    "            # Tenemos una lista de tensores de un unico elemento (el valor\n",
    "            # de la distancia). Para poder usar argmax pasamos todo esto\n",
    "            # a un unico tensor\n",
    "            positive_distances = torch.tensor(positive_distances)\n",
    "            negative_distances = torch.tensor(negative_distances)\n",
    "                        \n",
    "            # Calculamos la funcion de perdida\n",
    "            positives = self.list_of_classes[img_label]\n",
    "            negatives = self.list_of_negatives[img_label]\n",
    "            \n",
    "            worst_positive_idx = positives[torch.argmax(positive_distances)]\n",
    "            worst_negative_idx = negatives[torch.argmin(negative_distances)]\n",
    "            \n",
    "            worst_positive = embeddings[worst_positive_idx]\n",
    "            worst_negative = embeddings[worst_negative_idx]\n",
    "            \n",
    "            loss += self.base_loss(embedding, worst_positive, worst_negative)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def __precompute_list_of_classes(self, labels) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Calcula la lista con las listas de posiciones de cada clase por separado\n",
    "        \"\"\"\n",
    "        \n",
    "        # Inicializamos la lista de listas\n",
    "        posiciones_clases = [[] for _ in range(10)]\n",
    "\n",
    "        # Recorremos el dataset y colocamos los indices donde corresponde\n",
    "        for idx, label in enumerate(labels):\n",
    "            posiciones_clases[label].append(idx)\n",
    "\n",
    "        return posiciones_clases\n",
    "    \n",
    "    def __precompute_negative_class(self):\n",
    "        \n",
    "        # Inicializamos la lista\n",
    "        list_of_negatives = [None] * 10\n",
    "        \n",
    "        for label in range(10):\n",
    "            list_of_negatives[label] = [\n",
    "                idx\n",
    "                for current_list in skip_i(self.list_of_classes, label)\n",
    "                for idx in current_list\n",
    "            ]\n",
    "            \n",
    "        return list_of_negatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volvemos a cargar los datos\n",
    "\n",
    "- Con esta función de pérdida ya no necesitamos calcular de forma offline los triples\n",
    "- Así que volvemos a cargar el dataset original, sin el pre-cómputo de triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones que queremos aplicar al cargar los datos\n",
    "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
    "])\n",
    "\n",
    "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
    "# preparado para descargar\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Separamos train en train y validacion\n",
    "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
    "\n",
    "# Data loaders para acceder a los datos\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size = ONLINE_BATCH_SIZE,\n",
    "  shuffle = True,\n",
    "  num_workers = NUM_WORKERS,\n",
    "  pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18(\n",
      "  (pretrained): ResNet(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = ResNet18()\n",
    "\n",
    "# En este caso, al no estar trabajando con los minibatches\n",
    "# (los usamos directamente como nos los da pytorch), no tenemos\n",
    "# que manipular los tensores\n",
    "net.set_permute(False)\n",
    "\n",
    "# TODO -- fijar bien los parametros\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
    "parameters[\"lr\"] = 0.001\n",
    "parameters[\"criterion\"] = OnlineTripletLoss(MARGIN)\n",
    "\n",
    "# Definimos el logger que queremos para el entrenamiento\n",
    "logger = TripletLoggerOnline(\n",
    "    net = net,\n",
    "    iterations = 10 * ONLINE_BATCH_SIZE,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    "    train_percentage = 0.001,\n",
    "    validation_percentage = 0.1,\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Entrenamos solo si lo especifica\n",
    "# el parametro que controla el uso de cache\n",
    "if USE_CACHED_MODEL is False:\n",
    "\n",
    "    # Para saber cuanto tarda\n",
    "    ts = time.time()\n",
    "\n",
    "    training_history = train_model_online(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetworkOnline\",\n",
    "        logger = SilentLogger(),\n",
    "        snapshot_iterations = None\n",
    "    )\n",
    "\n",
    "    # Calculamos cuanto ha tardado\n",
    "    te = time.time()\n",
    "    print(f\"Ha tardado {te - ts}\")\n",
    "    \n",
    "    # Actualizamos la cache del modelo\n",
    "    filesystem.save_model(net, MODEL_CACHE_FOLDER, \"online_model_cached\")\n",
    "\n",
    "\n",
    "# Nos saltamos el entrenamiento y cargamos el modelo desde la cache\n",
    "else:\n",
    "    net = filesystem.load_model(os.path.join(MODEL_CACHE_FOLDER, \"online_model_cached\"), ResNet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamos solo si lo especifica\n",
    "# el parametro que controla el uso de cache\n",
    "if USE_CACHED_MODEL is False:\n",
    "\n",
    "    show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dc92ce37"
   },
   "source": [
    "# Evaluación del modelo\n",
    "\n",
    "- Mostramos algunas métricas fundamentales sobre el conjunto de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2242eedc"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "calculate_mean_triplet_loss_online() missing 1 required positional argument: 'max_examples'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-3f2ae59089ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"criterion\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Escritorio/5ºDGIIM/VC/Practicas/PracticaFinal/ComputerVisionPracticaFinal/core.py\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, test_loader, loss_function, online)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_mean_triplet_loss_online\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0monline\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_mean_triplet_loss_offline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Loss: {test_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: calculate_mean_triplet_loss_online() missing 1 required positional argument: 'max_examples'"
     ]
    }
   ],
   "source": [
    "core.test_model(net, test_loader, parameters[\"criterion\"], online = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptamos el modelo a clasificador y evaluamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4c53d41c5816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmbeddingToClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUMBER_NEIGHBOURS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f5ca6b7cd2c6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, embedder, k, data_loader)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Calculamos todos los embeddings de los puntos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_embedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__calculate_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Modelo de clasificacion k-nn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f5ca6b7cd2c6>\u001b[0m in \u001b[0;36m__calculate_embedding\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;31m# Calculamos el embedding de la imagen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mimg_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# Añadimos el embedding asociado a la etiqueta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-279e98f5b79b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Usamos directamente la red pre-entrenada para hacer el forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretrained\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    440\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 442\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    443\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-943b1f1a5a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_class\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mpredicted_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "source": [
    "# Hacemos esto y no `in test_dataset[:max_iterations]`\n",
    "# para no tener que tomar todo el dataset y quedarnos con\n",
    "# una parte de el, que es un proceso mucho mas lento que usar\n",
    "# el iterador que da `in test_dataset` y parar con el contador\n",
    "counter = 0\n",
    "max_iterations = 20\n",
    "\n",
    "for img, img_class in test_dataset:\n",
    "    predicted_class = classifier.predict(img)\n",
    "    print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
    "\n",
    "    # Actualizamos el contador\n",
    "    counter += 1\n",
    "    if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot del embedding\n",
    "\n",
    "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
    "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f644bbed"
   },
   "source": [
    "## Evaluación del clasificador obtenido\n",
    "\n",
    "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.embedder.set_permute(False)\n",
    "\n",
    "metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "pprint(metrics)\n",
    "\n",
    "classifier.embedder.set_permute(True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
